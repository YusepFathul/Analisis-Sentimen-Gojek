# -*- coding: utf-8 -*-
"""Projek_Data_Mining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10lQTdfvxQAwNVWQkUFt1wzTnQ-jnjfu9
"""

import pandas as pd
import numpy as np
import re

"""# **DATA PREPROCESSING**"""

!pip install kaggle

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d dewanakretarta/gojek-playstore-reviews

!unzip gojek-playstore-reviews.zip

data = pd.read_csv('gojek.csv')

data

data.isnull().sum()

data.dropna(inplace=True)

data.isnull().sum()

duplikat = data.duplicated()
print(duplikat)

data = data.drop(['reviewId', 'userName', 'userImage', 'thumbsUpCount','reviewCreatedVersion','at','replyContent', 'repliedAt','appVersion' ], axis=1, errors='ignore')
data

print(data.dtypes)

data

print(f"Jumlah baris awal: {len(data)}")

sampled_data = data.sample(n=10000, random_state=42)

print(f"Jumlah baris setelah sampling: {len(sampled_data)}")

sampled_data.to_csv('gojek_sampled.csv', index=False)

sampled_data

sampled_data.isnull().sum()

# Fungsi untuk membersihkan teks
def clean_text(text):
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    text = text.lower()
    return text



sampled_data['content'] = sampled_data['content'].apply(clean_text)

print(sampled_data)

"""# TOKENISASI"""

pip install stanza

#Tokenisasi menggunakan Stanza
import stanza

stanza.download('id')
nlp = stanza.Pipeline('id', processors='tokenize', batch_size=64)


def tokenize_batch(texts):
    docs = nlp("\n\n".join(texts))
    tokens_list = []


    current_index = 0
    for text in texts:
        doc_sentences = []
        while current_index < len(docs.sentences) and docs.sentences[current_index].text in text:
            doc_sentences.append(docs.sentences[current_index])
            current_index += 1
        tokens = [word.text for sent in doc_sentences for word in sent.words]
        tokens_list.append(tokens)

    return tokens_list

#Membersihkan dan tokenisasi data
sampled_data['content'] = sampled_data['content'].apply(clean_text)
batches = [sampled_data['content'][i:i + 64].tolist() for i in range(0, len(sampled_data), 64)]
tokenized_batches = []
for batch in batches:
    tokenized_batches.extend(tokenize_batch(batch))

sampled_data['content_tokens'] = tokenized_batches

#Menentukan kolom sentiment berdasarkan 'score'
def label_sentiment(score):
    if score < 3:
        return 'negatif'
    elif score == 3:
        return 'netral'
    else:
        return 'positif'

sampled_data['sentiment'] = sampled_data['score'].apply(label_sentiment)

sampled_data

"""#MODEL NAIVE BAYES"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE


X = sampled_data['content']
y = sampled_data['sentiment']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)


X_train_tokens = [" ".join(tokens) for tokens in tokenize_batch(X_train.tolist())]
X_test_tokens = [" ".join(tokens) for tokens in tokenize_batch(X_test.tolist())]


vectorizer = TfidfVectorizer(ngram_range=(1, 2))
X_train_vect = vectorizer.fit_transform(X_train_tokens)
X_test_vect = vectorizer.transform(X_test_tokens)


smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vect, y_train)


model = MultinomialNB(alpha=1.0)
model.fit(X_train_resampled, y_train_resampled)

# 12. Evaluasi model
y_pred = model.predict(X_test_vect)

# Tampilkan hasil evaluasi
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print(classification_report(y_test, y_pred))

# lihat akurasi model
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

"""# Visualisasi"""

#confusion matrix
cm = confusion_matrix(y_test, y_pred, labels=['negatif', 'netral', 'positif'])

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Negatif', 'Netral', 'Positif'],
            yticklabels=['Negatif', 'Netral', 'Positif'])

plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

#pie chart
sentiment_counts = sampled_data['sentiment'].value_counts()
plt.figure(figsize=(8, 6))
sentiment_counts.plot(kind='pie', autopct='%1.1f%%', colors=['#FF0000','#00FFFF','#ADFF2F'])
plt.title('Distribusi Sentimen')
plt.ylabel('')
plt.show()